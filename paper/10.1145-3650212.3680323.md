# Automated Program Repair via Conversation: Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT

- International Symposium on Software Testing and Analysis 2024

- [paper](https://dl.acm.org/doi/pdf/10.1145/3650212.3680323)

- Qwen2.5

- [ ] 人工修正

## 动机

本文的写作动机在于解决传统自动化程序修复技术存在的局限性。传统方法依赖于手工制作或挖掘的错误修复模式，导致生成的补丁种类有限，难以适应其他类型的错误与修复。此外，这些方法通常遵循“生成与验证”（Generate and Validate, G&V）的修复范式，即先生成大量的候选补丁，再逐一验证其有效性，这种方式不仅容易产生大量重复且无效的补丁，还忽视了测试失败信息及潜在有效补丁中可能包含的重要线索。为了解决这些问题，作者提出了ChatRepair——一种全新的基于对话的自动化程序修复方法，该方法通过在补丁生成过程中即时反馈来优化修复过程，从而提高修复的成功率和效率。具体来说，ChatRepair首先向大型语言模型提供相关测试失败的信息，然后根据先前补丁尝试的成功或失败情况不断调整生成策略，以生成更有效的补丁。这种方法不仅能够避免重复犯错，还能从早期成功中学习，进一步增加生成正确补丁的机会。

## 创新点

**维度**。我们开辟了一个新的维度，基于对话的范式，用于实现完全自动化的程序修复及其进一步应用。我们的工作首次展示了如何有效地利用以前被忽视的测试失败信息，以及早期的修补尝试，以对话的方式促使大语言模型（LLMs）生成更正确的修补代码。此外，我们展示了基于对话的LLM在自动程序修复（APR）中的广阔前景。

**技术**。我们开发了ChatRepair，一个基于对话的自动程序修复工具，使用了ChatGPT模型。更具体地说，我们自动提取有关初始测试失败和早期修补尝试的简明相关信息，借此提示ChatGPT进行有效的程序修复。

**评估**。我们在广泛研究的Defects4j 1.2、2.0 [31]、QuixBugs [39]和ConDefect [69]数据集上，评估了ChatRepair与当前最先进的基于学习的及传统的程序修复工具的表现。ChatRepair在Defects4j 1.2和2.0上分别获得了114个和48个正确的修复结果（比之前的最佳基线多出15个和4个）。此外，我们进行了广泛的消融研究，证明了利用丰富的语义测试失败信息和ChatRepair的对话范式，在程序修复中所带来的提升。

## 与现有方法的区别

- **利用测试失败信息进行动态反馈**：与传统方法不同，ChatRepair不仅在初始阶段使用错误代码作为输入，还会在每次尝试生成补丁后，利用详细的测试失败信息（如相关的测试代码和错误消息）进行反馈，这种动态反馈机制有助于更准确地定位问题所在，提高修复成功率。

- **从失败和成功案例中学习**：现有方法往往只关注生成的补丁是否通过测试，而忽略了从失败和成功案例中学习的重要性。ChatRepair则通过记录对话历史，从前一次尝试的失败中吸取教训，从成功案例中借鉴经验，从而在后续尝试中避免相同错误，提升补丁的质量。

- **采用对话式交互方式**：传统的自动化程序修复工具通常是一次性生成多个补丁，然后进行验证，而ChatRepair引入了对话式的交互方式，使得模型能够在修复过程中逐步调整自己的策略，这一过程更接近人类解决问题的方式，能够更好地应对复杂多变的修复任务。

- **充分利用大型语言模型的能力**：虽然之前也有研究尝试使用大型语言模型进行程序修复，但主要是通过静态的提示词生成补丁。ChatRepair则更进一步，充分利用了大型语言模型对复杂输入的理解能力和遵循指令执行任务的能力，特别是在处理未结构化或复杂的输入信息方面表现突出。


## 详细方法

首先，构建初始提示词。这个提示词包含了错误代码的位置、周围的上下文以及相关的测试失败信息。这些信息帮助大型语言模型（例如ChatGPT）理解需要修复的问题的本质。

然后，向大型语言模型发送构建好的提示词，请求生成第一个补丁。这个过程涉及到将错误代码片段中的错误行移除，并用特殊的填充标记(`<infill>`)代替，同时提供上下文（即错误代码前后的代码片段），以及从测试失败中提取的相关信息，指导模型生成正确的修复代码。

接下来，验证生成的补丁。将生成的补丁应用于原始代码，然后运行测试套件来检查修复是否成功。如果测试通过且该补丁尚未存在于已知的有效补丁列表中，则将其添加到列表中。

如果生成的补丁未能通过所有测试，或者尽管通过了测试但存在已知的合理补丁中，ChatRepair会将此次尝试的结果（包括补丁本身和相应的测试失败信息）与之前的对话历史结合起来，形成一个新的提示词，用于引导下一次补丁的生成。这样做可以让模型从之前的失败中学习，避免重复相同的错误。

随着对话的深入，ChatRepair会持续更新提示词，包括增加更多的背景信息和先前的尝试结果，以便模型能够基于更丰富的信息生成更好的补丁。为了防止输入超过模型的上下文窗口限制，设置了一个最大对话长度，一旦达到这个限制，就会重新从初始提示词开始新一轮的修复尝试。

最后，整个修复过程会设定一个最大尝试次数，当达到这个次数或找到有效的补丁时，修复过程结束。输出包括所有找到的合理补丁以及使用ChatGPT API的成本。

## 研究问题

**RQ1** ChatRepair与当前最先进的APR技术相比如何？

**RQ2** ChatRepair在不同修复场景中的表现如何？

**RQ3** ChatRepair的不同组件在提升修复效果方面的贡献是什么？

**RQ4** ChatRepair在ChatGPT训练数据截止日期之后，处理新增的近期错误时表现如何？

### 评价指标

**正确修复的数量 (Correct Fixes)** 这是指工具生成的补丁能够完全且正确地修复程序中的缺陷的数量。这是评估工具性能的最直接的指标。

**尝试次数 (Tries)** 这是指为了获得一个可工作的补丁，工具需要尝试的次数。这个指标反映了工具在生成补丁时的效率。

**美元成本 (Dollar Cost)** 这是指使用ChatGPT API来修复一个缺陷的成本。这个成本是根据ChatGPT的定价（$0.002 per 1000 tokens）来计算的。这个指标反映了工具的经济性。

### 每个研究问题的结果

**RQ1: ChatRepair与当前最先进的APR技术相比如何？**

实验设计
- 作者将ChatRepair与多种传统的、基于NMT的以及基于LLM的APR工具进行了比较。
- 使用的数据集包括Defects4j 1.2、Defects4j 2.0和QuixBugs。
- 对于每种修复场景（单行、单块、单函数），ChatRepair与BaseChatGPT（直接使用ChatGPT而不进行任何对话或反馈信息）和CodexRepair（最佳表现的LLM基APR工具）进行了比较。

实验结果：
- ChatRepair在Defects4j 1.2和2.0上分别实现了114和48个正确的修复，比之前的最佳工具分别多出了15和4个。
- 在QuixBugs数据集上，ChatRepair也展示了优于所有其他基线技术的性能。
- 通过利用对话和反馈信息，ChatRepair能够更有效地利用测试失败信息和之前的补丁尝试，从而提高了修复性能。

**RQ2: ChatRepair在不同修复场景中的表现如何？**

实验设计：
- 作者评估了ChatRepair在三种不同的修复场景下的表现：单行、单块和单函数。
- 对于每种场景，ChatRepair与BaseChatGPT和CodexRepair进行了比较。

实验结果：
- 在单行修复场景中，ChatRepair在Defects4j 1.2上实现了57个正确的修复，超过了BaseChatGPT和CodexRepair。
- 在单块修复场景中，ChatRepair在Defects4j 1.2上实现了76个正确的修复。
- 在单函数修复场景中，ChatRepair在Defects4j 1.2上实现了39个正确的修复。
- 这些结果表明，ChatRepair在不同的修复场景下都能提供有效的性能提升。

**RQ3: ChatRepair的不同组件在提升修复效果方面的贡献是什么？**

实验设计：
- 作者通过改变初始提示、反馈响应和最大对话长度来评估ChatRepair的不同组件对修复效果的影响。

实验结果：
- 初始提示中包含的测试失败信息（如测试名称、错误消息和失败行）对提高修复性能至关重要。
- 动态反馈响应（只有在新补丁与原始补丁有不同的失败时才提供反馈）比基础反馈更有效。
- 增加对话长度可以提高修复性能，但同时也会增加每次修复的成本。
- 通过结合这些组件，ChatRepair能够更有效地利用历史信息和反馈，从而提高修复效果。

**RQ4: ChatRepair在ChatGPT训练数据截止日期之后，处理新增的近期错误时表现如何？**

实验设计：
- 为了评估ChatRepair在ChatGPT训练数据截止日期之后处理新增错误的能力，作者使用了ConDefect数据集进行测试。
- ConDefect数据集包含了在ChatGPT训练数据截止日期之后发布的编程竞赛错误。

实验结果：
- ChatRepair在ConDefect数据集上显著优于基线技术，展示了其在处理新错误时的有效性。
- 与BaseChatGPT相比，ChatRepair在Java和Python版本上分别提高了42.9%和46.6%的正确修复数量。
- 这些结果表明，ChatRepair能够有效地利用测试失败信息和其他补丁尝试，即使在ChatGPT训练数据截止日期之后遇到的错误也能取得良好的修复效果。


## 有效性威胁

**内部有效性：**
- **手动验证的正确性**：正确性的验证依赖于手动检查，这可能会引入主观性和不一致性。尽管作者试图通过详细的代码审查来减少这种威胁，但完全自动化地验证补丁的正确性仍然是一个挑战。
- **数据泄露**：由于ChatGPT的训练数据可能包含参考补丁，这可能导致评估结果偏向于ChatRepair能够复现这些补丁。作者通过使用在ChatGPT训练数据截止日期之后收集的ConDefect数据集来评估ChatRepair，以减轻这种威胁。
- **完美故障定位的假设**：在评估中使用了完美的故障定位信息，这可能会影响评估结果。作者通过使用广泛采用的故障定位工具来减少这种威胁，并进行了额外的实验来展示ChatRepair在这种情况下的性能。

**外部有效性：**
- **评估数据集的代表性**：ChatRepair在特定数据集上的性能可能无法推广到其他类型的错误或不同的软件项目。作者通过在多个数据集上进行评估来展示其方法的通用性，包括Defects4j 2.0、两个QuixBugs数据集和ConDefect数据集。

## 未来展望

**改进ChatRepair的方法：**
- **处理多代码块错误**：当前的ChatRepair无法处理需要跨多个函数或文件进行修复的错误。作者计划通过使用LLM来识别潜在的多个错误位置，并开发检索代理来获取有用的项目特定信息，以改进ChatRepair处理多代码块错误的能力。
- **减少对传统故障定位工具的依赖**：作者计划通过使用LLM来识别潜在的bug位置，而不是完全依赖于传统的故障定位工具，以减少对它们的依赖。

**未来工作的方向：**
- **基于代理的调试方法**：作者希望他们的基于代理的调试方法能够解决上述限制，并激发使用LLM进行更全面的APR和调试的未来工作。
- **更广泛的软件工程任务**：虽然这项工作集中在APR上，但作者也计划探索使用LLM进行更广泛的软件工程任务，如代码审查、测试生成和程序优化。

**对LLM的进一步探索：**
- **对话和指令理解**：作者计划进一步探索LLM在对话和指令理解方面的能力，以提高APR的性能。
- **模型训练和微调**：作者可能会考虑对LLM进行微调和训练，以更好地适应程序修复任务，从而提高ChatRepair的性能。
