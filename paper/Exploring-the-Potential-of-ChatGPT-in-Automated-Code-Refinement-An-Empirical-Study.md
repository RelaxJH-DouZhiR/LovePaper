# Exploring the Potential of ChatGPT in Automated Code Refinement: An Empirical Study
- ICSE 2024

- [paper](https://arxiv.org/pdf/2309.08221)

- Qwen2.5 & ChatGPT4o

- [x] 人工修正

## 动机

本文旨在探索ChatGPT在自动化代码审查任务中的潜力。尽管ChatGPT在自然语言处理任务中表现出色，但在代码审查领域的表现尚不清楚。通过这项研究，作者希望填补这一空白，并提供关于如何利用大型语言模型提高代码质量的见解。

## 创新点

1. 进行了首次针对ChatGPT在基于代码审查评论的代码改进任务上的性能评估的实证研究。
2. 分析了ChatGPT在代码改进任务中面临的挑战，并提出了潜在的缓解策略。
3. 构建了一个新的高质量代码审查数据集，有助于未来的研究工作。

## 详细方法

<u>首先</u>，选择现有的基准测试CodeReview，并构建了一个新的高质量代码审查数据集。\
<u>然后</u>，使用最先进的代码审查工具CodeReviewer作为与ChatGPT比较的基线。\
<u>其次</u>，评估了不同提示和温度设置对ChatGPT在代码优化任务中精确匹配（EM）得分的影响，发现较低的温度设置可以产生更好、更稳定的结果，而在提示中描述代码审查场景有助于提高ChatGPT的表现。\
<u>最后</u>，创建了一个新的数据集，通过从未包含在标准基准中的仓库收集代码审查以及从包含在标准基准中的相同仓库收集近期的代码审查。

## 研究问题

1. ChatGPT在基于代码审查的自动代码优化任务中的性能如何？
2. 不同的提示和温度设置如何影响ChatGPT的性能？
3. ChatGPT在哪些方面表现良好，又在哪些方面存在挑战？

### 评价指标

EM(Exact Match)和BLEU(Bilingual Evaluation Understudy)得分被用作评估模型在代码优化任务中性能的主要指标。\
<u>EM强调生成内容与标准答案的完全一致性，而BLEU则关注生成内容与标准答案之间的相似度。</u>

### 每个研究问题的结果

1. 为了评估ChatGPT的性能，研究者们使用了新构建的数据集，并与最先进的代码审查工具CodeReviewer进行了比较。结果显示，ChatGPT在EM和BLEU得分上显著优于CodeReviewer。
2. 通过调整不同的提示和温度设置，研究者观察到这些因素对ChatGPT的EM得分有着显著的影响，最高可达5%和15%。
3. 对于ChatGPT的挑战，研究者识别出了由于缺乏领域知识、位置不明确及审查评论中的更改不清晰等原因导致的问题，并提出了改进审查质量等策略来解决这些问题。

## 有效性威胁

1. 基准模型和基准的选择可能对结果的有效性构成威胁，研究者通过选择最先进的方法作为参考并创建具有更严格筛选规则的新测试数据集CRN来解决这一问题。
2. ChatGPT预测的随机性也是另一个潜在的威胁，研究者通过在RQ1中每种设置运行十次来减轻这一问题，但由于访问ChatGPT API的成本较高，在RQ2中没有重复运行。
3. 提示设置可能构成威胁，因为可能存在不合适的提示设置影响结果。

## 未来展望

虽然ChatGPT在代码审查任务中展现出了有希望的结果，但仍有许多改进的空间。未来的研究方向包括自动生产高质量的代码审查、审查精细化以及自动检测和过滤低质量的代码审查。此外，研究还强调了现有评估指标（如EM和BLEU）的局限性，指出了开发更加准确可靠的评估模型性能指标的必要性。