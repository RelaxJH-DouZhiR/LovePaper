# Exploring the Potential of ChatGPT in Automated Code Refinement: An Empirical Study

- International Conference on Software Engineering 2024

- [paper](https://arxiv.org/pdf/2309.08221)

- Qwen2.5 & ChatGPT4o

- [x] 人工修正

## 动机

本文旨在探索ChatGPT在自动化代码审查任务中的潜力。尽管ChatGPT在自然语言处理任务中表现出色，但在代码审查领域的表现尚不清楚。通过这项研究，作者希望填补这一空白，并提供关于如何利用大型语言模型提高代码质量的见解。

## 创新点

1. 进行了首次针对ChatGPT在基于代码审查评论的代码改进任务上的性能评估的实证研究。
2. 分析了ChatGPT在代码改进任务中面临的挑战，并提出了潜在的缓解策略。
3. 构建了一个新的高质量代码审查数据集，有助于未来的研究工作。

## 详细方法

首先，设计实验评估ChatGPT的参数配置：
论文的研究首先通过实验来确定ChatGPT在代码精炼任务中的最佳参数配置。具体来说，作者选取了不同的提示词（Prompt）和温度（Temperature）设置，进行组合测试。提示词包括简单提示、场景描述、详细需求等多种形式，温度设置从0到2，作者选择了五种温度值来测试其对模型生成代码的影响。

然后，创建高质量的代码走查数据集：
作者使用现有的CodeReview基准数据集作为起点，但为了更全面地评估ChatGPT的表现，他们构建了一个新的高质量代码走查数据集，名为CodeReview-New。该数据集由两个部分组成：来自相同代码库但较新的代码走查评论（CodeReview-NewTime）和来自不同编程语言的代码走查评论（CodeReview-NewLanguage）。这些数据集用于测试模型的泛化能力。

接着，进行模型的训练与比较：
在实验中，ChatGPT模型和最先进的代码走查工具CodeReviewer进行了对比。使用ChatGPT时，作者通过提供代码走查评论和原始代码片段，让模型自动生成改进后的代码。为了评估ChatGPT在不同参数配置下的表现，作者通过训练和验证数据集对其进行了广泛的测试。

然后，对ChatGPT的表现进行定量评估：
使用Exact Match（EM）和BLEU作为主要的评价指标，作者对ChatGPT和CodeReviewer在不同数据集上的表现进行了评估。在标准数据集和新数据集上，ChatGPT的表现优于CodeReviewer，尤其是在泛化能力方面。ChatGPT在新数据集上的EM和BLEU分数显著高于CodeReviewer，表明ChatGPT在面对不同代码库和语言时有更好的适应性。

接下来，进行质性分析，探讨模型的优劣势：
作者通过手动标注的方式，对200个样本进行质性分析，深入探讨ChatGPT在代码精炼任务中的优势和不足。他们特别关注模型在处理不同评论质量和不同类型代码修改任务时的表现。结果表明，ChatGPT在处理高质量、具体的评论时表现最好，而在处理模糊或不明确的评论时效果较差。

最后，提出改进模型表现的策略：
作者通过对ChatGPT的失败案例进行分析，确定了模型表现不佳的主要原因，如领域知识的缺乏、评论内容不清楚等。为此，提出了两种改进策略：一是提高代码走查评论的质量，确保评论更加明确，二是使用更高级的大模型（如GPT-4）来应对更复杂的代码修改任务。通过实验验证，GPT-4在这些情况下的表现得到了显著提升。

## 研究问题

**RQ1.** *提示和温度对ChatGPT的影响如何？*
**RQ2.** ChatGPT在基于代码审查的自动代码优化任务中的性能如何？
**RQ3.** ChatGPT在哪些方面表现良好，又在哪些方面存在挑战？
**RQ4.** *ChatGPT为何表现不佳，根本原因和解决方案是什么？*

### 评价指标

EM(Exact Match)和BLEU(Bilingual Evaluation Understudy)得分被用作评估模型在代码优化任务中性能的主要指标。
*EM强调生成内容与标准答案的完全一致性，而BLEU则关注生成内容与标准答案之间的相似度。*

### 每个研究问题的结果

**RQ1.** *参数和温度的配置对 ChatGPT 在代码优化中的表现有显著影响。在大多数情况下，较低的温度设置往往会产生更好且更稳定的结果。包含简洁场景描述的提示通常会产生更好的效果。
研究者选出了 5 个效果最好、最具代表性的提示：
(1) Prompt 1 (P1)：最简单的提示。只提供了基于旧代码和评审生成新代码的基本要求，没有额外的描述。
(2) Prompt 2 (P2)：P1 + 场景描述。P2 基于 Prompt 1 设计，但包含了场景描述，要求 ChatGPT 扮演开发者角色，并根据来自团队领导的拉取请求中的评审信息修改代码。
(3) Prompt 3 (P3)：P1 + 详细要求。P3 包含了详细的要求信息，例如尽可能保持代码的原始内容和格式，不补全旧代码中的任何代码片段，也不修改评审中未提及的代码。
(4) Prompt 4 (P4)：P1 + 简明要求。类似于 P3，P4 也包含要求信息，但更为简洁。
(5) Prompt 5 (P5)：P4 + 场景描述。P5 是 Prompt 2 和 Prompt 4 的结合，包含了场景描述和要求信息。*

**RQ2.** 为了评估ChatGPT的性能，研究者们使用了新构建的数据集，并与最先进的代码审查工具CodeReviewer进行了比较。*结果显示，ChatGPT在应用于未见过的数据集时，表现出比CodeReviewer更好的泛化能力。然而，其有效性仍然有限，EM-trim和BLEU-trim分数仅为 22.78 和 76.55。*

**RQ3.** 该实验通过开发标注网站、两位作者独立标注并通过第三方讨论解决分歧，基于三个维度（评论相关性、评论信息和代码变更类别）对 CodeReview 数据集中的样本进行定性分析，以评估 ChatGPT 的优缺点，并通过 Cohen's Kappa 系数验证标注一致性。ChatGPT在包含具体建议的高质量评审上表现更好，而在相关性低且信息量少的评审中表现较差。此外，ChatGPT在代码重构任务上表现最佳，而在涉及文档和功能优化的任务上表现较低。

**RQ4.** *研究者分析中发现的主要根本原因是缺乏领域知识、位置不明确以及变更不清晰。为缓解这些问题，确定了两个潜在方向：一是改进大型语言模型，例如使用 GPT-4 代替 GPT-3.5，二是提高评审质量，提供更清晰的信息。*

## 有效性威胁

1. 基准模型和基准的选择可能对结果的有效性构成威胁，研究者通过选择最先进的方法作为参考并创建具有更严格筛选规则的新测试数据集CRN来解决这一问题。
2. ChatGPT预测的随机性也是另一个潜在的威胁，研究者通过在RQ1中每种设置运行十次来减轻这一问题，但由于访问ChatGPT API的成本较高，在RQ2中没有重复运行。
3. 提示设置可能构成威胁，因为可能存在不合适的提示设置影响结果。

## 未来展望

虽然ChatGPT在代码审查任务中展现出了有希望的结果，但仍有许多改进的空间。未来的研究方向包括自动生产高质量的代码审查、审查精细化以及自动检测和过滤低质量的代码审查。此外，研究还强调了现有评估指标（如EM和BLEU）的局限性，指出了开发更加准确可靠的评估模型性能指标的必要性。