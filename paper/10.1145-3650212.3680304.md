# DistillSeq: A Framework for Safety Alignment Testing in Large Language Models using Knowledge Distillation

- International Symposium on Software Testing and Analysis 2024

- [paper](https://dl.acm.org/doi/pdf/10.1145/3650212.3680304)

- ChatGPT4o

- [ ] 人工修正

## 动机

随着大型语言模型（LLMs）的快速发展和广泛应用，这些模型在教育和内容创作等领域展现出强大的能力。然而，这种进步也带来了显著的伦理问题，尤其是LLMs可能被恶意利用来生成欺骗性或破坏性的内容。例如，通过使用所谓的“越狱提示”（jailbreak prompt），恶意用户可以绕过LLMs内置的安全机制，使其产生有害信息。鉴于此，研究者们致力于开发有效的测试框架，以评估和确保LLMs的安全性和合规性。然而，对LLMs进行广泛的测试需要大量的计算资源，这使得测试过程既耗时又昂贵。因此，探索能够节省成本的测试策略变得至关重要，以平衡彻底评估的需求与资源限制之间的矛盾。本文提出了一种名为DistillSeq的新方法，旨在通过知识蒸馏技术将LLMs中的审核知识转移到小型模型中，并设计了一套自动化测试框架来生成恶意查询，随后利用蒸馏后的模型过滤出有效的测试案例，从而有效减少与LLMs不必要的或无成效的交互，优化测试流程。

## 创新点

1. 知识蒸馏用于大型语言模型的安全对齐：我们提出了一种新颖的知识蒸馏方法，将大型语言模型的审核知识转移到更小、更高效的蒸馏模型中。
2. 有效的测试框架：我们提出了一种名为DistillSeq的自动化测试框架，旨在利用语法树和大型语言模型生成恶意查询。这些查询随后通过蒸馏模型进行过滤，以识别出用于测试大型语言模型的有效查询。
3. 方法有效性的实证证据：我们在四个主流大型语言模型上评估了DistillSeq。结果表明，DistillSeq显著提高了平均攻击成功率93%，并生成了比现有方法更有效的恶意查询。

## 与现有方法的区别

- 成本效益：与传统直接与LLMs交互的测试方法相比，DistillSeq通过使用知识蒸馏技术，显著降低了测试成本，同时保持了较高的测试效率。
- 自动化程度：DistillSeq框架实现了恶意查询生成和过滤的自动化过程，而之前的许多研究主要依赖于手动构造恶意查询。
- 测试效率：通过引入过滤器，DistillSeq能够更有效地筛选出有效的恶意查询，减少了无效测试的数量，提高了测试的整体效率。
- 适用范围：DistillSeq不仅适用于特定的LLMs，还展示了在多种LLMs上的良好性能，具有较强的通用性和扩展性。

## 详细方法

- **首先，知识蒸馏阶段**：
    - 选择一个大型语言模型（LLM）作为教师模型，并准备一个较小的模型作为学生模型。
    - 使用教师模型生成大量标注数据，包括正常查询和恶意查询及其相应的响应。
    - 将这些标注数据用于训练学生模型，使学生模型学习到教师模型的审核机制和安全对齐知识。
    - 训练完成后，学生模型将具备识别和过滤恶意查询的能力，但计算成本远低于教师模型。

- **然后，恶意查询生成阶段**：
    - **语法树方法**：
      - 随机选择两个潜在的恶意查询。
      - 将这两个查询解析成语法树结构。
      - 计算每个语法子树的重要性分数。
      - 收集另一个查询中的语法子树。
      - 替换第一个查询中重要性最高的子树，使用相同语法类别的子树进行替换，生成新的恶意查询。
      - 通过这种方法，生成的恶意查询在语义上更加多样，同时保持语法正确性。
    - **基于LLM的方法**：
      - 使用一个预训练的LLM生成恶意查询。
      - 通过输入特定的模板或提示，引导LLM生成具有潜在危害性的查询。
      - 这些查询经过进一步筛选和优化，确保它们能够有效测试LLM的安全性。

- **接着，过滤阶段**：
    - 使用训练好的学生模型作为过滤器，对生成的恶意查询进行初步评估。
    - 学生模型会判断这些查询是否有可能引发有害响应。
    - 过滤掉那些不太可能引起有害响应的查询，保留高风险的查询。

- **最后，测试阶段**：
    - 将过滤后的恶意查询发送给教师模型进行测试。
    - 记录教师模型对这些查询的响应，评估其是否生成了有害内容。
    - 通过比较有无过滤器的情况下的攻击成功率，验证过滤器的有效性。

## 研究问题

5. **逐条列出论文的研究问题**：

- **RQ1: DistillSeq的有效性**：DistillSeq框架能否有效利用知识蒸馏从大型语言模型（LLMs）中学习，准确分类可能产生有毒内容的查询？

- **RQ2: 数据集大小与模型性能的权衡**：在知识蒸馏过程中，数据集大小与模型性能之间的权衡是什么？如何选择最优的数据集大小以实现成本效益？
- **RQ3: 方法的通用性**：DistillSeq框架在不同的LLMs和新数据集上的表现如何？
- **RQ4: 恶意查询生成方法的比较**：基于语法树的方法和基于LLM的方法在生成恶意查询方面的效果如何？哪种方法更有效？
- **RQ5: 过滤器的有效性**：过滤器在减少无效测试用例和提高测试效率方面的作用如何？

### 评价指标

6. **介绍在论文中所使用的全部评价指标**：

- **Attack Success Rate (ASR)**：
   - **定义**：攻击成功率为有效查询数量占总查询数量的比例。
   - **用途**：评估不同方法生成的恶意查询在引发LLM生成有害内容方面的效果。

- **Filtered Attack Success Rate (FASR)**：
   - **定义**：过滤后的攻击成功率为通过过滤器的有效查询数量占通过过滤器的总查询数量的比例。
   - **用途**：评估过滤器在提高恶意查询有效性和减少无效测试用例方面的效果。

- **Agreement**：
   - **定义**：一致性是指蒸馏模型与LLM在分类恶意查询方面的吻合度。
   - **用途**：评估蒸馏模型在学习和模仿LLM审核机制方面的效果。

- **Loss**：
   - **定义**：损失值是模型在训练过程中的误差指标，通常用于衡量模型的预测与真实标签之间的差距。具体计算方法取决于所使用的损失函数，例如交叉熵损失。
   - **用途**：评估蒸馏模型在训练过程中的性能，确保其能够准确地学习LLM的审核知识。

- **Average Attack Success Rate (AASR)**：
   - **定义**：平均攻击成功率为所有测试LLMs的攻击成功率的平均值。
   - **用途**：评估不同方法在多个LLMs上的整体表现，确保其在不同模型上的普遍有效性。

- **Cost Savings**：
   - **定义**：成本节约是指使用DistillSeq框架相对于传统方法在测试成本上的节省。
   - **用途**：评估DistillSeq框架在提高测试效率和降低成本方面的效果。

### 每个研究问题的结果

- RQ1: DistillSeq的有效性：
  - **数据集**：使用包含恶意查询的数据集，这些查询已知能够引发有害响应。
  - **模型选择**：选择四个流行的LLMs（GPT-3.5、GPT-4.0、Vicuna-13B、Llama-13B）作为教师模型。
  - **知识蒸馏**：训练一个小型模型（学生模型），使其学习教师模型的审核机制。
  - **评估指标**：使用Attack Success Rate (ASR) 和 Filtered Attack Success Rate (FASR) 评估学生模型的性能。
  - **实验结果**：
    - 在没有DistillSeq的情况下，各LLM的攻击成功率分别为：GPT-3.5 31.5%，GPT-4.0 21.4%，Vicuna-13B 28.3%，Llama-13B 30.9%。
    - 应用DistillSeq后，攻击成功率显著提高：GPT-3.5 58.5%，GPT-4.0 50.7%，Vicuna-13B 58.25%，Llama-13B 60.20%。
    - 结论：DistillSeq框架显著提高了恶意查询的有效性，验证了其在学习和模仿LLMs审核机制方面的有效性。

- RQ2: 数据集大小与模型性能的权衡：
  - **数据集大小**：分别使用不同大小的数据集（例如1000、2000、3000、4000个样本）进行知识蒸馏。
  - **模型训练**：训练学生模型，并在相同的测试集上评估其性能。
  - **评估指标**：使用Agreement和Loss评估不同数据集大小下的模型性能。
  - **实验结果**：
    - 随着数据集大小的增加，学生的模型性能逐渐提高，但在4000个样本时达到最佳平衡点。
    - 4000个样本时，学生的模型在Agreement和Loss方面的表现最佳，同时保持了较低的训练成本。
    - 结论：4000个样本是一个合理的选择，能够在保证模型性能的同时，实现成本效益。

- RQ3: 方法的通用性：
  - **数据集**：使用Jigsaw Toxic Comment Classification Challenge数据集中的1000个查询进行评估。
  - **模型选择**：选择四个流行的LLMs（GPT-3.5、GPT-4.0、Vicuna-13B、Llama-13B）。
  - **评估指标**：使用Agreement评估学生模型在新数据集上的性能。
  - **实验结果**：
    - 学生模型在新数据集上的Agreement范围为52.10%到71.80%，其中DeBERTa表现出最高的平均Agreement。
    - 结论：DistillSeq框架在不同LLMs和新数据集上均表现出良好的性能，验证了其通用性和适应性。

- RQ4: 恶意查询生成方法的比较：
  - **生成方法**：分别使用随机词替换、TextFooler、JailbreakingLLMs、GPT-Fuzzer、FuzzLLM、HouYi等方法生成1000个恶意查询。
  - **过滤器**：使用DeBERTa模型作为过滤器，过滤无效的恶意查询。
  - **评估指标**：使用ASR和FASR评估不同生成方法的效果。
  - **实验结果**：
    - 基于LLM的方法结合过滤器的ASR最高，平均达到58.25%。
    - 语法树方法结合过滤器的ASR次之，平均达到49.78%。
    - 其他方法（如随机词替换和TextFooler）的ASR均低于20%。
    - 结论：基于LLM的方法结合过滤器在生成恶意查询方面最为有效，显著提高了测试效率。

- RQ5: 过滤器的有效性：
  - **过滤器**：使用DeBERTa模型作为过滤器，过滤无效的恶意查询。
  - **评估指标**：使用ASR和FASR评估过滤器在减少无效测试用例和提高测试效率方面的效果。
  - **实验结果**：
    - 使用过滤器后，所有方法的ASR均有显著提高，表明过滤器有效减少了无效测试用例。
    - 例如，GPT-3.5的ASR从31.5%提高到58.5%，GPT-4.0的ASR从21.4%提高到50.7%。
    - 结论：过滤器在减少无效测试用例和提高测试效率方面发挥了重要作用，验证了其在实际测试中的有效性。

## 有效性威胁

- 内部有效性威胁：
  - 随机性：LLMs对相同有害内容的响应存在不一致性和随机性，这可能影响实验结果的可靠性。为了应对这一问题，研究者对每个有害输入进行了多次查询。
  - 模型选择：研究中仅选择了四种LLMs进行实验，其他模型可能表现更好。因此，该方法在不同LLMs上的表现需要进一步验证和适应。
- 外部有效性威胁：
  - 模型泛化能力：虽然研究涵盖了多种广泛使用的LLMs，但不同模型在架构和训练方法上存在差异，因此该方法在其他LLMs上的有效性可能需要特定的调整。
  - 数据集限制：研究中使用了RealToxicityPrompts数据集，这可能限制了模型处理多样化恶意查询的能力。为了解决这一问题，研究者在RQ2中引入了Jigsaw数据集，以增强模型的适应性。

## 未来展望

- 神经信息访问：
作者计划探索是否可以通过访问神经信息来开发性能更高、效率更好的过滤器。这可能涉及到对模型内部机制的更深入理解，从而提高过滤器的准确性和效率。
- 多模态和跨语言扩展：
未来的研究可以探索DistillSeq框架在多模态和跨语言场景中的应用。这将有助于提高LLMs在不同模态和语言环境下的安全性和鲁棒性。
- 防御策略：
作者强调了进一步研究和开发防御策略的重要性，以增强LLMs对抗日益复杂的恶意查询的能力。这包括开发新的防御机制和技术，以提高LLMs的安全性。
- 更广泛的模型和应用场景：
未来的工作可以扩展到更多的LLMs和应用场景中，验证DistillSeq框架的通用性和适应性。这将有助于推动LLMs在更多领域的安全应用。
- 社区合作和开源：
作者鼓励社区合作，共同推进LLMs的安全对齐测试技术的发展。开放源代码和共享研究成果将有助于加速这一领域的进步。
