# Automated Unit Test Improvement using Large Language Models at Meta

- Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering 2024

- [paper](https://dl.acm.org/doi/pdf/10.1145/3663529.3663839)

- GPT-4o

- [ ] 人工修正

## 动机

论文的写作动机是通过开发和部署名为 TestGen-LLM 的工具，利用大规模语言模型（LLM）来自动改进现有人类编写的单元测试，提高测试覆盖率和质量，同时避免由于 LLM 假想引起的问题。论文的主要目标是解决工业规模软件测试的自动化问题，为测试生成和代码改进提供有保障的验证机制。这项工作特别关注大型工业环境中的测试需求，如 Instagram 和 Facebook 的测试类覆盖和质量提升。

## 创新点

1. 首个关于基于可信大语言模型的软件工程（Assured LLM-based Software Engineering，简称 Assured LLMSE）的实例引入。特别是，我们认为这是首次报道由大语言模型生成的代码完全独立于人工干预（除了最终的审查和批准），并成功部署到大规模工业生产系统中的案例，同时对现有代码库提供了保证性的改进。
2. 在对 Instagram 的 Reels 和 Stories 产品进行评估时，TestGen-LLM 生成的测试用例中，有 75% 能够成功构建，57% 能稳定通过测试，25% 提升了覆盖率。
3. 一份关于 Meta 在 2023 年的开发、部署和演进的定性和定量结果报告。当用于逐步提升 Instagram 和 Facebook 生产测试类的覆盖率时，TestGen-LLM 能够改进其所应用的所有测试类的 10%，并且 73% 的测试改进被开发人员接受并最终部署到生产环境。
4. 描述了通过将 Assured LLMSE 应用于软件测试改进中所获得的经验教训、未解决的问题以及由此提出的研究挑战。

## 与现有方法的区别

1. **改进现有测试**：
   - **现有方法**：许多现有的测试生成工具主要集中在从头开始生成测试用例。
   - **TestGen-LLM**：专注于改进现有的测试类，通过生成额外的测试用例来覆盖未被现有测试覆盖的边缘情况，从而增加整体测试覆盖率。

2. **保证改进**：
   - **现有方法**：大多数现有方法在生成测试用例后，可能缺乏对生成结果的严格验证，导致生成的测试用例可能与现有测试重复或无效。
   - **TestGen-LLM**：使用一系列过滤器确保生成的测试用例不仅能够构建和通过，而且能够提供新的代码覆盖率，避免重复和无效的测试用例。

3. **自动化与保证**：
   - **现有方法**：虽然一些现有方法也使用自动化技术生成测试用例，但往往缺乏对生成结果的自动验证和保证机制。
   - **TestGen-LLM**：采用“Assured LLM-based Software Engineering”（Assured LLMSE）方法，确保生成的测试用例在提交给工程师审查之前已经通过了严格的验证，提供了可验证的改进保证。

4. **多模型集成**：
   - **现有方法**：通常依赖单一的语言模型生成测试用例。
   - **TestGen-LLM**：使用多个语言模型和不同的提示策略，通过集成学习的方法，结合不同模型和提示的结果，提高生成测试用例的质量和多样性。

5. **增量部署**：
   - **现有方法**：许多现有方法在实际应用中直接大规模部署，可能导致各种不可预见的问题。
   - **TestGen-LLM**：采用了逐步增量部署的方式，从初始试验到最小可行产品（MVP），再到大规模部署，每个阶段都进行了详细的评估和优化，确保系统的稳定性和可靠性。

6. **用户体验**：
   - **现有方法**：生成的测试用例可能不符合开发者的编码风格和习惯，导致接受度较低。
   - **TestGen-LLM**：生成的测试用例遵循现有的编码风格和命名规范，提高了开发者的接受度。此外，还提供了详细的覆盖率报告，帮助开发者更好地理解和评估生成的测试用例。

## 详细方法

**第一步，初始输入与上下文获取**  
- 获取现有的人类编写的测试类（existing test class），以及与之相关的被测试类（class under test）。  
- 根据输入的具体上下文，决定采用何种提示（prompt）策略，如扩展测试类的覆盖率（extend_coverage）或聚焦边界情况（corner_cases）。

**第二步，语言模型生成候选测试用例**  
- 将输入的测试类及被测试类作为提示，传递给语言模型（LLM），以生成多个候选测试用例。  
- 使用不同的模型（如 LLM1 和 LLM2）、提示策略（如 extend_test 等）以及超参数（如温度值）来生成多样化的候选测试用例。  

**第三步，构建检查过滤（Buildability Check）**  
- 检查生成的候选测试用例是否能够在目标代码库中通过构建。  
- 任何无法通过构建的候选测试用例将被丢弃，不进入后续步骤。

**第四步，可靠性测试过滤（Reliability Filter）**  
- 执行生成的候选测试用例，并检查其是否能可靠通过所有运行。  
- 如果测试用例在多次运行中结果不一致（即为 flaky 测试），则将其丢弃。

**第五步，覆盖率评估过滤（Coverage Improvement Filter）**  
- 使用覆盖率工具（如 JaCoCo）计算每个测试用例对测试类的覆盖率改进情况。  
- 如果候选测试用例未能增加覆盖率，则被丢弃；只有增加覆盖率的测试用例才能进入最终结果。

**第六步，多模型协作与结果整合（Ensemble Approach Integration）**  
- 将通过上述所有过滤的测试用例按不同模型、提示和参数策略分类，进行去重和整合。  
- 每个测试用例都提供可验证的改进证明（如覆盖率增量）和详细文档。

**第七步，测试类的重新构建与优化**  
- 将通过过滤的测试用例重新整合到现有的测试类中，形成改进后的完整测试类。  
- 确保生成的测试类与原有测试类的风格一致（如命名规则、注释风格和断言方法）。

**第八步，工具的自动化运行与工程师验证**  
- 将改进后的测试类提交给工程师进行审查。工程师可以选择接受或拒绝工具的建议。  
- 工具在运行过程中生成详细的日志和覆盖率报告，供工程师验证和分析。

**第九步，反馈和优化**  
- 工程师的反馈会用于调整提示策略、超参数和过滤标准。  
- 工具会在后续的运行中结合新的反馈信息，进一步提高生成质量和工业实用性。

### 评价指标
 
**1. 构建通过率 (Build Success Rate)**  
- **定义**：生成的测试用例是否能够成功编译和构建。  
- **作用**：评估生成的测试用例是否符合代码语法规范，是所有后续过滤的基础。  
- **实验结果**：成功通过构建的测试用例被保留，失败的用例直接被丢弃。

**2. 测试通过率 (Test Pass Rate)**  
- **定义**：生成的测试用例是否在运行时通过所有测试而不失败。  
- **作用**：验证测试用例的功能逻辑是否正确，以及是否适用于现有系统。  
- **实验应用**：用于筛选功能正确的测试用例，未通过的用例被丢弃。

**3. 波动性过滤 (Flakiness Filter)**  
- **定义**：通过多次重复运行测试用例（通常 5 次），检查其结果是否一致。  
- **作用**：消除运行结果不稳定的测试用例（“波动测试”）。  
- **实验意义**：确保测试用例的可靠性，防止工具引入不稳定因素。

**4. 覆盖率改进 (Coverage Improvement)**  
- **定义**：生成的测试用例是否增加了代码的行覆盖率。  
- **作用**：作为主要的改进衡量指标，记录生成的测试用例在覆盖率上的实际贡献。  
- **实验结果**：只有覆盖率有所改进的用例才会被保留。

**5. 成功率 (Success Rate)**  
- **定义**：生成的测试用例通过所有过滤步骤（构建、运行、可靠性和覆盖率）的比例。  
- **作用**：综合衡量工具的总体性能，结合所有核心指标的结果。  
- **实验应用**：用于对比不同平台、模型、提示策略和参数配置的效果。

**6. 平均覆盖率增长 (Average Coverage Growth)**  
- **定义**：成功生成的测试用例平均增加的代码行覆盖率数量。  
- **作用**：评估每个生成用例的增量贡献，分析工具在覆盖率提升上的边际收益。

**7. 测试类改进率 (Test Class Improvement Rate)**  
- **定义**：TestGen-LLM 应用到的测试类中，成功改进覆盖率或质量的比例。  
- **作用**：反映工具在测试类改进任务中的覆盖范围和效果。  
- **实验结果**：在实验中，工具成功改进了约 10% 的测试类。

**8. 测试改进接受率 (Test Improvement Acceptance Rate)**  
- **定义**：开发者在代码审查中接受 TestGen-LLM 提供的测试改进建议的比例。  
- **作用**：验证工具生成测试用例的工业适用性和开发者认可度。  
- **实验结果**：开发者接受了约 73% 的工具生成测试改进建议。

**9. 覆盖范围细节 (Coverage Details)**  
- **定义**：生成测试用例的覆盖范围，包括：  
  - 新增覆盖的文件数量。  
  - 新增覆盖的代码行数。  
  - 增加的分支覆盖率（如适用）。  
- **作用**：深入评估工具生成用例的覆盖效果，确保改进的覆盖范围和深度。

### 每个研究问题的结果

**研究问题 1：TestGen-LLM 在不同平台上的表现如何？**

**实验方法：**
1. TestGen-LLM 被部署在 Instagram 和 Facebook 两个平台上，这两个平台的测试用例库和代码库复杂度有所不同。
2. 记录两个平台上生成测试用例的总尝试次数（trials），以及通过所有过滤步骤的成功测试用例数量（successful trials）。

**实验结果：**
- **Instagram 平台**：
  - 总尝试次数：23,535 次。
  - 成功测试用例：831 个。
  - 成功率：4%。
- **Facebook 平台**：
  - 总尝试次数：8,996 次。
  - 成功测试用例：490 个。
  - 成功率：5%。

**结论**：Facebook 平台的成功率略高，作者推测这可能与 Facebook 平台有更多人类编写的 Kotlin 测试用例（训练数据）相关。

**研究问题 2：不同温度设置对生成测试用例成功率的影响是什么？**

**实验方法：**
1. TestGen-LLM 在实验中采用不同的温度设置（从 0.0 到 0.9，以 0.1 为步长）生成测试用例。
2. 比较不同温度设置下测试用例的成功率（通过所有过滤步骤的比例）。
3. 分析温度对生成测试用例多样性和质量的影响。

**实验结果：**
- 温度为 0.0 时：
  - 总尝试次数：30,483。
  - 成功测试用例：1,215。
  - 成功率：4%。
- 温度为 0.4 时：
  - 总尝试次数：334。
  - 成功测试用例：16。
  - 成功率：5%。

**结论**：
- 尽管 0.4 的成功率略高，但由于默认温度 0.0 的测试次数更多，仍被选为默认设置。
- 高温度设置（如 0.8 和 0.9）的成功率较低（约 1-3%），可能是因为高温度带来的生成结果随机性较高。

**研究问题 3：两种 LLM 模型（LLM1 和 LLM2）的性能如何？**

**实验方法：**
1. 在实验中测试两种语言模型（LLM1 和 LLM2）生成的测试用例，通过相同的过滤步骤评估每种模型的成功率。
2. 比较每种模型在 Facebook 和 Instagram 平台上的表现。

**实验结果：**
- **总体表现**：
  - LLM1：总尝试次数 3,173，成功 163 个，成功率 5%。
  - LLM2：总尝试次数 28,654，成功 1,157 个，成功率 4%。
- **平台差异**：
  - Facebook 平台：
    - LLM1 成功率：7%（总尝试 719）。
    - LLM2 成功率：5%（总尝试 8,146）。
  - Instagram 平台：
    - LLM1 成功率：5%（总尝试 2,454）。
    - LLM2 成功率：3%（总尝试 20,508）。

**结论**：
- LLM1 和 LLM2 的总体成功率接近，但 LLM1 在 Facebook 平台表现更优，LLM2 的默认模型覆盖更多尝试。

**研究问题 4：TestGen-LLM 的整体覆盖率改进表现如何？**

**实验方法：**
1. 在 TestGen-LLM 的部署中统计生成的测试用例是否能够通过过滤步骤并增加代码覆盖率。
2. 分析覆盖率增长的范围和分布。

**实验结果：**
1. **测试类改进率**：
   - TestGen-LLM 应用到的 1,979 个测试类中，有 196 个测试类被成功改进。
   - 改进率约为 10%。
2. **测试改进接受率**：
   - 开发者对改进建议的接受率为 73%。
3. **覆盖率增长的对比**：
   - 覆盖率增长曲线具有对数特性：随着生成测试用例数量的增加，覆盖率的增速逐渐下降。

**结论**：
- 在规模化的部署中，TestGen-LLM 能够实现覆盖率的稳定提升，尤其是在初期阶段覆盖率增长明显。

## 有效性威胁

**1. 覆盖率作为改进指标的局限性**
- **问题**：论文使用代码行覆盖率作为主要的改进衡量指标，但行覆盖率无法全面反映测试用例的有效性或漏洞检测能力。
- **影响**：可能导致生成的测试用例虽然增加了覆盖率，但在实际功能测试或错误检测中的价值有限。
- **建议改进**：可以引入更强的覆盖率指标，例如分支覆盖率或突变覆盖率（mutation coverage）。

**2. 对测试用例质量的进一步评估不足**
- **问题**：生成的测试用例通过覆盖率和运行结果来评估质量，但未对其逻辑合理性或断言有效性进行更深入的验证。
- **影响**：可能会生成形式上正确但功能上意义有限的测试用例。
- **建议改进**：增加自动化断言生成或语义验证工具来提升生成测试的质量。

**3. 模型依赖性和可扩展性**
- **问题**：方法依赖于 Meta 内部开发的 LLM 模型（LLM1 和 LLM2），未明确说明其性能是否能在其他模型上复现。
- **影响**：如果模型在工业以外环境中的性能不同，可能影响其通用性。
- **建议改进**：评估工具在其他公开可用 LLM（如 GPT、CodeT5 等）上的表现，以提高方法的通用性。

**4. 对工业代码风格依赖**
- **问题**：工具高度依赖现有代码的风格（如命名规则和注释风格）来生成一致性较高的测试用例。
- **影响**：在风格混乱或不一致的代码库中，工具可能生成不符合预期风格的测试用例，降低其实用性。
- **建议改进**：引入更智能的代码风格学习与适应模块，以应对风格多样化的代码库。

**5. 对人类审查的依赖**
- **问题**：生成的测试用例需要开发者进行最终审查和选择，这在一定程度上依赖人工干预。
- **影响**：在人力资源有限的情况下，可能降低工具的部署效率。
- **建议改进**：开发更强大的自动化评价机制，减少对人工审查的依赖。

**6. 未完全解决测试“波动性”问题**
- **问题**：尽管使用了多次运行过滤波动性测试，但对某些复杂的波动原因（如依赖外部环境的代码）未完全解决。
- **影响**：可能仍会生成部分偶然通过的测试用例。
- **建议改进**：结合静态分析工具，提前识别和避免生成波动性较高的测试用例。

**7. 模型生成重复性测试**
- **问题**：实验发现，LLM 模型可能生成几乎完全重复的测试用例，仅在命名上略有不同。
- **影响**：浪费计算资源，生成冗余的测试用例。
- **建议改进**：加入更智能的语义去重机制，例如基于 Type-2 或 Type-3 克隆检测的技术，过滤重复的测试。

**8. 工具的覆盖率增长呈现递减趋势**
- **问题**：覆盖率的增长在测试用例生成的后期逐渐减少，显示边际收益递减的现象。
- **影响**：在覆盖率已经较高的代码库中，工具的实际价值可能会降低。
- **建议改进**：针对覆盖率较高的代码库，优化提示策略（如专注于未覆盖的复杂逻辑或边界情况）。

**9. 工具的适用性受训练数据限制**
- **问题**：实验表明，工具在拥有更多人类编写测试用例的平台（如 Facebook）上表现更优。
- **影响**：如果代码库缺乏丰富的训练数据或测试用例，工具的性能可能会下降。
- **建议改进**：结合迁移学习或跨领域学习技术，增强工具在低资源环境下的适应能力。

## 未来展望

**1. 改进覆盖率度量标准**  
- **现状**：当前使用代码行覆盖率作为测试改进的主要衡量指标，但这一指标无法全面反映测试用例的有效性。  
- **展望**：未来计划引入更强的覆盖率度量标准，例如：
  - **突变覆盖率**（Mutation Coverage）：利用突变测试衡量测试用例对错误的检测能力。  
  - **分支覆盖率**（Branch Coverage）：捕捉更复杂的逻辑路径覆盖情况。  

**2. 应用场景定制化的概率分布优化**  
- **现状**：语言模型生成测试用例时，未针对具体应用场景优化概率分布。  
- **展望**：探索基于不同应用场景调整生成策略的方法，以提高模型生成的针对性和效率。例如：
  - 针对复杂逻辑的代码生成更具针对性的测试用例。  
  - 通过优化提示策略和模型超参数适应特定领域的需求。

**3. 解决语言模型的“风格盲从”问题**  
- **现状**：语言模型倾向于模仿现有代码风格，但可能保留了过时或不一致的风格。  
- **展望**：未来需要：
  - 设计方法检测和纠正不良编码习惯或风格。  
  - 在模型生成过程中动态调整，生成符合现代化标准的测试代码。  

**4. 加强静态分析与语言模型的结合**  
- **现状**：语言模型生成的测试用例更多基于概率推断，缺乏对代码结构的深入分析。  
- **展望**：结合静态分析技术，与语言模型生成能力互补。例如：
  - 提高代码覆盖率分析的精确性。  
  - 检测生成代码中的语义重复或潜在问题。

**5. 自动化测试的多维度改进目标**  
- **现状**：当前主要关注测试覆盖率的改进。  
- **展望**：扩展到其他代码改进目标，例如：
  - **性能改进**：生成能够提升运行效率的测试用例。  
  - **安全性验证**：针对代码中的潜在安全隐患生成测试用例。  
  - **健壮性测试**：生成能够测试系统异常行为的用例。

**6. 提高生成测试用例的解释性与透明性**  
- **现状**：生成的测试用例虽然提供了覆盖率增量的详细数据，但对生成逻辑的解释仍有限。  
- **展望**：未来可以：
  - 为生成的测试用例提供详细的生成过程解释和语义分析报告。  
  - 增强开发者对语言模型生成逻辑的理解和信任。

**7. 扩展至更广泛的应用领域**  
- **现状**：当前实验集中于 Instagram 和 Facebook 的代码库。  
- **展望**：将方法扩展到其他代码库和语言，包括：
  - 跨平台支持更多的编程语言（如 Python、JavaScript 等）。  
  - 应用于更广泛的行业场景（如医疗、金融等领域的软件开发）。

**8. 动态和在线测试生成**  
- **现状**：TestGen-LLM 当前以离线方式生成测试用例。  
- **展望**：未来计划探索动态和在线测试生成技术，例如：
  - 根据代码的实时更改生成相应的测试用例。  
  - 在持续集成系统中动态推荐测试用例，进一步提高效率。

**9. 改善生成过程的效率与资源利用**  
- **现状**：生成和过滤测试用例需要一定的计算资源，尤其是在大规模代码库中。  
- **展望**：通过优化生成流程或分布式计算，进一步提高工具的效率，减少资源消耗。

**10. 跨团队和社区的合作**  
- **现状**：TestGen-LLM 的开发和部署主要在 Meta 内部完成。  
- **展望**：与学术界和其他工业团队合作，共同解决更广泛的问题，例如：
  - 开发开源版本以便更广泛的社区使用和改进。  
  - 探讨跨组织协作的可能性，优化不同代码库的应用效果。
