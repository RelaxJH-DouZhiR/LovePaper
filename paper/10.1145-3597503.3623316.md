# Chain-of-thought prompting elicits reasoning in large language models

- International Conference on Software Engineering 2024

- [paper](https://arxiv.org/pdf/2302.00288)

- Qwen2.5 & ChatGPT4o

- [ ] 人工修正

## 动机

该论文的写作动机是为了弥补当前代码生成基准测试中存在的不足，特别是现有的基准测试（如HumanEval和AiXBench）仅关注独立函数的生成，而没有包括非独立函数的生成。然而，在实际的开源项目中，非独立函数占据了超过70%的比例。论文提出了一个新的基准测试“CoderEval”，用于评估生成模型在现实代码生成环境中的效果，特别是解决依赖于外部上下文的非独立函数生成问题。

## 创新点

- *我们通过分析100个最受欢迎的用Java和Python编写的开源项目，指出了现有基准的局限性：现有基准（如HumanEval）通常只包含独立函数，而在开源项目中，非独立函数占比超过70%。*
- *我们提出了CoderEval，一个面向实用代码生成的基准。CoderEval源自多个领域的开源项目，考虑了非原始类型、第三方库和项目特定的上下文引用。此外，CoderEval还包括人工标注的目标函数的文档字符串，以补充原始文档字符串。*
- *我们在CoderEval上评估并比较了三种最先进的代码生成模型（CodeGen、PanGu-Coder和ChatGPT）。实验结果揭示了三个重要发现：(1) 这些模型在非独立函数上的表现不如在独立函数上的表现，(2) 对所有这些模型而言，生成具有上下文依赖性的代码既重要又具有挑战性，即使是最强大的ChatGPT也是如此，(3) 使用人工标注的文档字符串与原始文档字符串对代码生成效果有影响。*

## 与现有方法的区别

- 现有方法的局限性：现有的代码生成模型评估基准，如HumanEval和AiXBench，主要关注于评估独立函数的生成能力。这些基准中的函数通常只调用或访问内置函数和标准库，忽略了在实际开发中更为常见的非独立函数。根据论文的研究，超过70%的开源项目中的函数是非独立函数，这意味着现有基准无法全面反映模型在实际场景下的表现。
- CoderEval的创新点：为了解决上述问题，本文提出了CoderEval，这是一个面向实用代码生成的评估基准。它包含了从各种领域的真实开源项目中精心挑选的230个Python问题和230个Java问题。这些问题不仅涵盖了独立函数，更重要的是包括了具有上下文依赖性的非独立函数，这些上下文可能涉及到类型、API、变量和常量等元素，这些元素定义在目标函数之外，但在第三方库、当前类、文件或项目中有定义。此外，CoderEval还提供了一个基于Docker的自包含执行平台，用于自动评估生成代码的功能正确性，这使得评估过程更加贴近实际开发环境。
- 评估指标的扩展：除了传统的Pass@K度量外，CoderEval还引入了一个新的度量标准Acc@K，以更全面地评估生成代码的质量。通过这一系列改进，CoderEval能够更好地评估模型在处理实际开发任务时的表现，尤其是对于那些需要考虑上下文信息的任务。

## 详细方法

首先，研究团队从GitHub上爬取了所有项目的标签，并选择了最频繁出现的14个标签以及拥有高星数的项目。这14个标签包括“gson”、“音乐”、“日志记录”、“聊天”、“WebSocket”、“MVC”、“LeetCode”、“微服务”、“JDBC”、“JSON”、“CRUD”、“数据结构”、“Log4j”和“序列化”。对于每个标签，选取了星数最高的前五名项目作为候选项目。

然后，从选定的项目中提取所有的函数，但排除了测试、接口或已废弃的函数，并且保留了有英文函数级注释的函数。同时，确保这些函数能够在验证平台上成功运行并通过原有的测试案例。

接着，通过人工筛选的方式选择高质量的函数。主要标准是该函数是否经常出现在真实的开发场景中。为了保证质量，研究团队制定了五个规则来指导函数的选择，例如函数应包含一定的逻辑复杂性和代码行数等。

随后，根据每个项目中所选函数的数量来确定最终入选的项目。这一过程有助于减少需要编译的项目数量，同时保持总选中函数的数量不变。最终，从10个Java项目中获得了230个问题，为了与Java版本的问题数量保持一致，还从43个Python项目中获取了230个问题。

为了减轻数据泄露的风险（即原始文档字符串可能已经被大型语言模型用作训练数据），研究团队招募了13位经验丰富的工程师，为每个问题提供了一个人工标注的描述版本，作为第二个文档字符串来补充原始的文档字符串。

最后，研究团队对每个项目提供的现有测试进行了测试覆盖率的检查，并手动编写了额外的测试用例以达到较高的测试覆盖率，从而提高了评估的准确性。通过这种方式，CoderEval能够提供一个更贴近实际、多样化的评估基准，用于评估代码生成模型在真实开发场景中的有效性。

## 研究问题

- **RQ1**: 如何评估CodeGen、PanGu-Coder和ChatGPT这三个模型在生成独立函数与非独立函数方面的性能？
- **RQ2**: 这些模型在正确地将oracle_context信息（如类型引用、API调用和变量引用）纳入生成代码中的表现如何？
- **RQ3**: 不同的提示（如原始文档字符串和人工标注的文档字符串）如何影响这些模型的有效性？

### 评价指标

- **Pass@K**: 这是一个广泛采用的度量标准，用于衡量给定230个问题中，至少有一个正确生成的解决方案（通过运行相应的测试用例来判断）出现在由LLM生成的前K个样本中的问题百分比。实验中设置n（生成的样本总数）为10，并分别计算K值为1、5和10时的Pass@K。
- **Acc@K**: 这是论文中提出的一个新度量标准，基于oracle_context令牌（即包含在oracle_context信息中的依赖元素）来评估LLM生成每个单独oracle_context令牌的能力。具体而言，Acc@K衡量的是，在230个目标函数（作为230个问题的正确解）中，每个oracle_context令牌至少被K个样本之一包含的目标函数的百分比。此外，还可以制定其他度量标准来评估LLM生成包含所有oracle_context令牌的函数的能力，考虑令牌的顺序与否。这些度量标准对LLM提出了更高的要求。

### 每个研究问题的结果


**RQ1**: 如何评估CodeGen、PanGu-Coder和ChatGPT这三个模型在生成独立函数与非独立函数方面的性能？
- 实验设计：
  - 使用CoderEval和HumanEval两个基准对三个模型进行评估。
  - 在CoderEval中，评估对象既包括独立函数也包括非独立函数。非独立函数进一步分为四个可执行级别：plib-runnable（标准库可运行）、class-runnable（类可运行）、file-runnable（文件可运行）和project-runnable（项目可运行）。
  - 每个模型针对每个问题生成10个代码片段，使用Pass@K（K=1, 5, 10）来评估模型生成的代码片段中有多少能通过测试用例。
- 实验结果：
  - ChatGPT在大多数可执行级别上的Pass@5和Pass@10指标上表现最佳，尤其是在Java的非独立函数生成方面。
  - 三个模型在生成独立函数上的效果明显优于非独立函数。
  - 各模型在生成独立函数时表现出色，而在处理非独立函数时则面临挑战，尤其是当涉及到更复杂的上下文依赖时。
  - 实验还显示了不同模型之间的互补性和重叠性。在CoderEval for Python中，三个模型共同解决了91个问题，其中32个问题是三个模型都能解决的；在CoderEval for Java中，三个模型共同解决了136个问题，其中56个问题是三个模型都能解决的。

**RQ2**: 这些模型在正确地将oracle_context信息（如类型引用、API调用和变量引用）纳入生成代码中的表现如何？
- 实验设计：
  - 分析生成的代码中oracle_context令牌的准确性，包括TypeReference（类型引用）、APIInvocation（API调用）和VarReference（变量引用）三类令牌。
  - 使用Acc@K（K=1, 5, 10）来评估生成的代码中包含正确oracle_context令牌的比例。
- 实验结果：
  - 所有模型在生成Python代码时，对TypeReference令牌的准确性最高，而对VarReference令牌的准确性最低。
  - 对于Java代码，所有模型在生成TypeReference和APIInvocation令牌时表现较好，但在生成VarReference令牌时表现较差。
  - Acc@K值与Pass@K值呈现出一致性，表明生成的函数能够通过测试用例与正确生成oracle_context令牌之间存在相关性。

**RQ3**: 不同的提示（如原始文档字符串和人工标注的文档字符串）如何影响这些模型的有效性？
- 实验设计：
  - 比较使用原始文档字符串和人工标注的文档字符串作为提示时，模型生成代码的效果差异。
  - 使用Pass@K（K=1, 5, 10）和Acc@K（K=1, 5, 10）作为评估指标。
- 实验结果：
  - 人工标注的文档字符串在某些情况下可以提高模型生成代码的准确性，特别是在生成包含复杂oracle_context信息的代码时。
  - 不同模型对提示的敏感度不同。例如，ChatGPT在使用人工标注的文档字符串时表现更好，而其他模型在使用原始文档字符串时也能达到较好的效果。
  - 总体而言，提供更准确和详细的提示可以显著提高模型生成代码的质量，特别是在处理非独立函数时。

## 有效性威胁

- **模型代表性**：实验中使用的三个模型（CodeGen、PanGu-Coder和ChatGPT）均来自工业界，并且每个模型在其首次提出时都在HumanEval上取得了当时最先进的性能。然而，由于作者在撰写论文时计算资源有限，没有对其他最近提出的模型（如CodeGeex、StarCoder和Wizard-Coder）进行实验。这种限制可能导致实验结果不能完全代表当前所有模型的性能水平。未来的工作可以通过对更多类型的模型进行实验来减少这一威胁。

- **统计数据的准确性**：关于独立函数与非独立函数比例的统计可能存在偏差。为了识别一个函数是否依赖于第三方库，作者收集了Python的PyPI和Java的Maven中的所有第三方库。但由于无法保证收集到所有第三方库，因此可能导致统计结果略有偏差。未来可以通过更全面的数据收集来提高统计的准确性。
- **评估指标的局限性**：虽然CoderEval引入了Acc@K这一新的评估指标，但其主要还是基于Pass@K和Acc@K这两种指标来评估模型的性能。这些指标可能无法全面反映模型在实际开发中的表现，特别是对于一些复杂的应用场景。未来可以探索更多的评估指标，以更全面地评估模型的实用性。
- **实验设置的局限性**：虽然CoderEval从多个开源项目中精心挑选了问题，但这些项目的选择和问题的选取标准可能仍然存在主观性。此外，实验中每个问题仅生成10个代码片段，这可能不足以充分评估模型在大规模生成任务中的表现。未来可以通过增加生成样本的数量和多样性来进一步验证模型的性能。
- **上下文信息的复杂性**：CoderEval虽然引入了oracle_context信息来评估模型生成非独立函数的能力，但这些上下文信息的复杂度可能仍然有限。在实际开发中，代码往往涉及更复杂的上下文依赖，如跨文件的变量引用、多模块的交互等。未来的工作可以进一步探索如何在评估基准中包含更复杂的上下文信息，以更好地模拟实际开发场景。